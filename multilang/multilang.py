import pandas as pd
import torch
import numpy as np
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
import os
import glob

# ==================== ç®€åŒ–çš„è®­ç»ƒæ¨¡å— ====================

class SimpleTrainer:
    def __init__(self, model, tokenizer, train_dataset, val_dataset, learning_rate=2e-5, batch_size=8):
        self.model = model
        self.tokenizer = tokenizer
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        self.model.to(self.device)
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
        
    def train(self, epochs=3):
        print("å¼€å§‹è®­ç»ƒ...")
        self.model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            train_loader = torch.utils.data.DataLoader(
                self.train_dataset, batch_size=self.batch_size, shuffle=True
            )
            
            for batch_idx, batch in enumerate(train_loader):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                
                if batch_idx % 20 == 0:
                    print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')
            
            avg_loss = total_loss / len(train_loader)
            print(f'Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}')
            
            # æ¯ä¸ªepochåéªŒè¯
            eval_results = self.evaluate()
            print(f'Epoch {epoch+1} éªŒè¯ç»“æœ: {eval_results}')
    
    def evaluate(self):
        self.model.eval()
        val_loader = torch.utils.data.DataLoader(
            self.val_dataset, batch_size=self.batch_size
        )
        
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                probs = torch.sigmoid(outputs.logits)
                preds = (probs > 0.5).int().cpu().numpy()
                labels_np = labels.cpu().numpy()
                
                all_preds.extend(preds)
                all_labels.extend(labels_np)
        
        f1_macro = f1_score(all_labels, all_preds, average='macro')
        f1_micro = f1_score(all_labels, all_preds, average='micro')
        
        return {'f1_macro': f1_macro, 'f1_micro': f1_micro}
    
    def predict(self, test_dataset):
        self.model.eval()
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=self.batch_size)
        
        all_preds = []
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )
                
                probs = torch.sigmoid(outputs.logits)
                preds = (probs > 0.5).int().cpu().numpy()
                all_preds.extend(preds)
        
        return np.array(all_preds)

# ==================== æ•°æ®é›†ç±» ====================

class PolarizationDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text, 
            truncation=True, 
            padding='max_length',
            max_length=self.max_length, 
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label, dtype=torch.float)
        }

# ==================== é¢„æµ‹å¤šä¸ªæ•°æ®é›†çš„å‡½æ•° ====================

def predict_multiple_datasets(trainer, tokenizer, test_files, label_columns, output_dir='/kaggle/working/'):
    """å¯¹å¤šä¸ªæµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹ï¼Œæ¯ä¸ªæ•°æ®é›†è¾“å‡ºå•ç‹¬çš„CSVæ–‡ä»¶"""
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    all_predictions = {}
    
    for dataset_name, file_path in test_files.items():
        print(f"\næ­£åœ¨å¤„ç†æµ‹è¯•é›†: {dataset_name}")
        print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        
        try:
            # è¯»å–æµ‹è¯•æ•°æ®
            test_df = pd.read_csv(file_path)
            print(f"æˆåŠŸè¯»å– {len(test_df)} æ¡æµ‹è¯•æ•°æ®")
            
            # æ•°æ®é¢„å¤„ç† - ç¡®ä¿åˆ—é¡ºåºæ­£ç¡®
            new_column_order = ['id', 'political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']
            
            def reorder_columns(df, new_order):
                existing = [col for col in new_order if col in df.columns]
                others = [col for col in df.columns if col not in new_order]
                return df[existing + others]
            
            test_df = reorder_columns(test_df, new_column_order)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
            test_dataset = PolarizationDataset(
                test_df['text'].tolist(),
                np.zeros((len(test_df), 5)),  # æµ‹è¯•é›†æ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œç”¨0å¡«å……
                tokenizer
            )
            
            # è¿›è¡Œé¢„æµ‹
            print(f"å¼€å§‹å¯¹ {dataset_name} è¿›è¡Œé¢„æµ‹...")
            test_preds = trainer.predict(test_dataset)
            
            # åˆ›å»ºç»“æœDataFrame
            results_df = test_df[['id']].copy()
            for i, col in enumerate(label_columns):
                results_df[col] = test_preds[:, i]
            
            # ä¿å­˜åˆ°å­—å…¸ä¸­
            all_predictions[dataset_name] = results_df
            
            # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”Ÿæˆå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶å
            output_filename = f"predictions_{dataset_name}.csv"
            output_path = os.path.join(output_dir, output_filename)
            
            # ä¿å­˜åˆ°ç‹¬ç«‹çš„CSVæ–‡ä»¶
            results_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
            # æ˜¾ç¤ºæ ·æœ¬ç»“æœå’Œç»Ÿè®¡
            print(f"\n{dataset_name} é¢„æµ‹ç»“æœç¤ºä¾‹:")
            print(results_df.head(3))
            
            print(f"\n{dataset_name} é¢„æµ‹æ ‡ç­¾ç»Ÿè®¡:")
            for col in label_columns:
                count = int(results_df[col].sum())
                percentage = (count / len(results_df)) * 100
                print(f"  {col}: {count} æ¡ ({percentage:.1f}%)")
                
        except Exception as e:
            print(f"âŒ å¤„ç† {dataset_name} æµ‹è¯•é›†æ—¶å‡ºé”™: {e}")
            continue
    
    return all_predictions

# ==================== ä¸»æµç¨‹ ====================

def main():
    # å®‰è£…å¿…è¦çš„åº“
    try:
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
    except ImportError:
        print("å®‰è£…transformersåº“...")
        os.system("pip install transformers")
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
    
    # è®­ç»ƒé›†æ–‡ä»¶æ˜ å°„
    train_files = {
        'eng': '/kaggle/input/semeval/train_eng.csv',
        'amh': '/kaggle/input/semeval/train_amh.csv',
        'arb': '/kaggle/input/semeval/train_arb.csv',
        'deu': '/kaggle/input/semeval/train_deu.csv',
        'fas': '/kaggle/input/semeval/train_fas.csv',
        'hau': '/kaggle/input/semeval/train_hau.csv',
        'hin': '/kaggle/input/semeval/train_hin.csv',
        'ita': '/kaggle/input/semeval/train_ita.csv',
        'nep': '/kaggle/input/semeval/train_nep.csv',
        'spa': '/kaggle/input/semeval/train_spa.csv',
        'tur': '/kaggle/input/semeval/train_tur.csv',
        'urd': '/kaggle/input/semeval/train_urd.csv',
        'zho': '/kaggle/input/semeval/train_zho.csv'
    }
    
    # æµ‹è¯•é›†æ–‡ä»¶æ˜ å°„ - å¯ä»¥æ·»åŠ å¤šä¸ªæµ‹è¯•é›†
    test_files = {
        'zho': '/kaggle/input/semeval/dev_zho.csv',
        'eng': '/kaggle/input/semeval/dev_eng.csv',
        'arb': '/kaggle/input/semeval/dev_arb.csv',
        'deu': '/kaggle/input/semeval/dev_deu.csv',
        'fas': '/kaggle/input/semeval/dev_fas.csv',
        'hin': '/kaggle/input/semeval/dev_hin.csv',
        'spa': '/kaggle/input/semeval/dev_spa.csv',
        'hau': '/kaggle/input/semeval/dev_hau.csv',
        'amh': '/kaggle/input/semeval/dev_amh.csv',
        'ita': '/kaggle/input/semeval/dev_ita.csv',
        'nep': '/kaggle/input/semeval/dev_nep.csv',
        'tur': '/kaggle/input/semeval/dev_tur.csv',
        'urd': '/kaggle/input/semeval/dev_urd.csv',
        # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæµ‹è¯•é›†
    }
    
    # è‡ªåŠ¨æ£€æµ‹æµ‹è¯•é›†æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    def auto_detect_test_files(base_path='/kaggle/input/semeval/'):
        """è‡ªåŠ¨æ£€æµ‹æµ‹è¯•é›†æ–‡ä»¶"""
        detected_files = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰dev_*.csvæ–‡ä»¶
        dev_files = glob.glob(os.path.join(base_path, 'dev_*.csv'))
        for file_path in dev_files:
            # æå–æ•°æ®é›†åç§°
            filename = os.path.basename(file_path)
            dataset_name = filename.replace('.csv', '')  # ä¾‹å¦‚: dev_zho
            detected_files[dataset_name] = file_path
            
        # æŸ¥æ‰¾æ‰€æœ‰test_*.csvæ–‡ä»¶
        test_files = glob.glob(os.path.join(base_path, 'test_*.csv'))
        for file_path in test_files:
            # æå–æ•°æ®é›†åç§°
            filename = os.path.basename(file_path)
            dataset_name = filename.replace('.csv', '')  # ä¾‹å¦‚: test_eng
            detected_files[dataset_name] = file_path
            
        return detected_files
    
    # ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹çš„æµ‹è¯•é›†æ–‡ä»¶ï¼ˆå–æ¶ˆæ³¨é‡Šä»¥å¯ç”¨ï¼‰
    # print("è‡ªåŠ¨æ£€æµ‹æµ‹è¯•é›†æ–‡ä»¶...")
    # test_files = auto_detect_test_files()
    # print(f"æ£€æµ‹åˆ° {len(test_files)} ä¸ªæµ‹è¯•é›†")
    
    all_data = []
    
    # å¤„ç†æ¯ç§è®­ç»ƒè¯­è¨€ - ç›´æ¥è¯»å–ä¸ç¿»è¯‘
    for lang, file_path in train_files.items():
        print(f"\n{'='*50}")
        print(f"å¤„ç† {lang} è®­ç»ƒæ•°æ®...")
        print(f"{'='*50}")
        
        try:
            df = pd.read_csv(file_path)
            print(f"åŸå§‹æ•°æ®: {len(df)} æ¡")
            all_data.append(df)
            print(f"å®Œæˆ {lang}")
            
        except Exception as e:
            print(f"å¤„ç† {lang} æ—¶å‡ºé”™: {e}")
            continue
    
    # åˆå¹¶æ•°æ®
    if all_data:
        combined_train = pd.concat(all_data, ignore_index=True)
    else:
        print("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•è®­ç»ƒæ•°æ®ï¼")
        return
    
    print(f"\næœ€ç»ˆè®­ç»ƒé›†: {len(combined_train)} æ¡")
    
    # æ•°æ®é¢„å¤„ç†
    new_column_order = ['id', 'political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']
    
    def reorder_columns(df, new_order):
        existing = [col for col in new_order if col in df.columns]
        others = [col for col in df.columns if col not in new_order]
        return df[existing + others]
    
    combined_train = reorder_columns(combined_train, new_column_order)
    
    # åˆ†å‰²æ•°æ®é›†
    train, val = train_test_split(combined_train, test_size=0.2, random_state=42)
    
    print(f"\næ•°æ®é›†å¤§å°:")
    print(f"è®­ç»ƒé›†: {len(train)}")
    print(f"éªŒè¯é›†: {len(val)}")
    print(f"æµ‹è¯•é›†æ•°é‡: {len(test_files)}")
    
    # åŠ è½½å¤šè¯­è¨€æ¨¡å‹ - ä½¿ç”¨XLM-RoBERTa
    print("\nåŠ è½½å¤šè¯­è¨€æ¨¡å‹...")
    model_name = "xlm-roberta-base"  # æ”¯æŒ100ç§è¯­è¨€çš„å¤šè¯­è¨€æ¨¡å‹
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=5,
            problem_type="multi_label_classification"
        )
        print(f"æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
    except Exception as e:
        print(f"åŠ è½½å¤šè¯­è¨€æ¨¡å‹å¤±è´¥: {e}")
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å¤šè¯­è¨€BERT
        print("å°è¯•åŠ è½½å¤šè¯­è¨€BERT...")
        tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-multilingual-cased",
            num_labels=5,
            problem_type="multi_label_classification"
        )
    
    # åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
    label_columns = ['political', 'racial/ethnic', 'religious', 'gender/sexual', 'other']
    
    train_dataset = PolarizationDataset(
        train['text'].tolist(),
        train[label_columns].values.tolist(),
        tokenizer
    )
    
    val_dataset = PolarizationDataset(
        val['text'].tolist(),
        val[label_columns].values.tolist(),
        tokenizer
    )
    
    # è®­ç»ƒæ¨¡å‹
    trainer = SimpleTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        learning_rate=2e-5,
        batch_size=8
    )
    
    trainer.train(epochs=3)
    
    # å¯¹å¤šä¸ªæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    print("\n" + "="*60)
    print("å¼€å§‹å¯¹å¤šä¸ªæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    print("="*60)
    
    all_predictions = predict_multiple_datasets(trainer, tokenizer, test_files, label_columns)
    
    # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*60)
    print("é¢„æµ‹æ±‡æ€»æŠ¥å‘Š")
    print("="*60)
    
    summary_data = []
    for dataset_name, pred_df in all_predictions.items():
        row = {'æ•°æ®é›†': dataset_name, 'æ ·æœ¬æ•°é‡': len(pred_df)}
        for col in label_columns:
            count = int(pred_df[col].sum())
            percentage = (count / len(pred_df)) * 100
            row[col] = f"{count} ({percentage:.1f}%)"
        summary_data.append(row)
    
    summary_df = pd.DataFrame(summary_data)
    print(summary_df)
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_file = '/kaggle/working/predictions_summary.csv'
    summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
    print(f"\næ±‡æ€»æŠ¥å‘Šä¿å­˜åˆ°: {summary_file}")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ‰€æœ‰é¢„æµ‹æ–‡ä»¶
    print("\n" + "="*60)
    print("ç”Ÿæˆçš„é¢„æµ‹æ–‡ä»¶åˆ—è¡¨")
    print("="*60)
    
    prediction_files = glob.glob('/kaggle/working/pred_*.csv')
    for file_path in prediction_files:
        file_size = os.path.getsize(file_path) / 1024  # KB
        print(f"ğŸ“„ {os.path.basename(file_path)} ({file_size:.1f} KB)")

if __name__ == "__main__":
    main()